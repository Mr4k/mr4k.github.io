<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peter Stefek</title>
    <description>I&#39;m a rising third year student at Oberlin College. I am currently majoring in Math and Computer Science. 
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 25 Aug 2016 23:19:10 +0200</pubDate>
    <lastBuildDate>Thu, 25 Aug 2016 23:19:10 +0200</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Out of My Depth (Part 2)</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/matrix_transform.png&quot; /&gt; 
&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Aug 2016 14:01:00 +0200</pubDate>
        <link>http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/20/sfm.html</link>
        <guid isPermaLink="true">http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/20/sfm.html</guid>
        
        
        <category>structure</category>
        
        <category>from</category>
        
        <category>motion</category>
        
        <category>depth</category>
        
        <category>linear</category>
        
        <category>algebra</category>
        
      </item>
    
      <item>
        <title>Out of My Depth (Part 1)</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/headache.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;In computer vision the structure from motion problem is centered around reconstructing a 3d scene from a set of two dimensional images. I’ve always found this idea to be really cool. The app &lt;a href=&quot;http://www.123dapp.com/catch&quot;&gt;123d Catch&lt;/a&gt; by Autodesk is great for playing around with these ideas, allowing you to build 3D models by taking pictures of an object on your phone.&lt;/p&gt;

&lt;p&gt;There are two distinct parts of the structure from motion problem.&lt;/p&gt;

&lt;p&gt;The first part is called feature mapping. Given a set of two dimensional images, the first thing we need to do is identify which parts of one image correspond to parts of the others. This article will not address feature mapping but if you are interested in how it works I would recommend looking at the SIFT or FAST algorithms.&lt;/p&gt;

&lt;p&gt;In this series of articles we will address the second part of the structure from motion problem. Suppose we have two images, I&lt;sub&gt;1&lt;/sub&gt; and I&lt;sub&gt;2&lt;/sub&gt; . We also have some points p&lt;sub&gt;1&lt;/sub&gt; in I&lt;sub&gt;1&lt;/sub&gt; which correspond to the points p&lt;sub&gt;2&lt;/sub&gt; in I&lt;sub&gt;2&lt;/sub&gt;. How can we find the depth of each point?&lt;/p&gt;

&lt;p&gt;To begin we have to talk about cameras. The simplest camera is called a pinhole camera. You’ve probably seen one before in elementary school science class.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/pinhole.gif&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Pinhole Camera Model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For this algorithm we will assume the camera through which I&lt;sub&gt;1&lt;/sub&gt; and I&lt;sub&gt;2&lt;/sub&gt; were taken is a perfect pinhole camera. This means that the pinhole is really small. We will refer to the back of the camera as the image plane. Because the pinhole is infinitesimally small we know that for any 2d point on the image plane (p&lt;sub&gt;x&lt;/sub&gt;, p&lt;sub&gt;y&lt;/sub&gt;) the corresponding 3d point, (P&lt;sub&gt;x&lt;/sub&gt;, P&lt;sub&gt;y&lt;/sub&gt;, P&lt;sub&gt;z&lt;/sub&gt;), must lie along on a ray which starts at the origin with direction (-p&lt;sub&gt;x&lt;/sub&gt;, -p&lt;sub&gt;y&lt;/sub&gt;, 1). Note that we assume the pinhole is the origin of our the 3d coordinate system. Now we will define the depth of a particular point (λ) as the distance from that point to the origin. All of this is summarized by the following equation (note that we drop a negative sign for simplicity so the sign of λ will flip):&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;(p&lt;sub&gt;x&lt;/sub&gt;, p&lt;sub&gt;y&lt;/sub&gt;, 1) = (P&lt;sub&gt;x&lt;/sub&gt;, P&lt;sub&gt;y&lt;/sub&gt;, P&lt;sub&gt;z&lt;/sub&gt;)
&lt;/p&gt;
&lt;p&gt;In vector notation this becomes:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;p = P
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Two Camera Model&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/twocamera.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Let’s assume we have two perfect pinhole cameras C&lt;sub&gt;1&lt;/sub&gt;, C&lt;sub&gt;2&lt;/sub&gt; each defined by a point in space and the direction it faces. Now for simplicity center the coordinate system around C&lt;sub&gt;1&lt;/sub&gt;, such that it’s center is at the origin and it points straight down the z-axis. Now we know that we there exists an affline transformation (a rotation R and a translation T) which will center the coordinate system around C&lt;sub&gt;2&lt;/sub&gt;. So for any point p in the image plane of C&lt;sub&gt;1&lt;/sub&gt; we can relate it to its corresponding point in the image plane of C&lt;sub&gt;2&lt;/sub&gt; with the equation:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;Rp&lt;sub&gt;2&lt;/sub&gt; + T
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Complications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So you might be thinking that if we had enough points we could set up a system of equations and solve for the unknown variables above. The problem with this approach is that the above equation is not linear because λ&lt;sub&gt;2&lt;/sub&gt;Rp&lt;sub&gt;2&lt;/sub&gt; contains the product of two unknowns (R and λ&lt;sub&gt;2&lt;/sub&gt;). This makes it very difficult to solve. However in the next article we will use some clever math turn this into a system we can solve.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Aug 2016 13:00:50 +0200</pubDate>
        <link>http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/11/sfm2.html</link>
        <guid isPermaLink="true">http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/11/sfm2.html</guid>
        
        
        <category>structure</category>
        
        <category>from</category>
        
        <category>motion</category>
        
        <category>depth</category>
        
        <category>linear</category>
        
        <category>algebra</category>
        
      </item>
    
      <item>
        <title>Regress the Chess</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/training.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Despite the lack of posts recently I have not given up on the chess engine but it will definitely take longer than I thought. Recently I decide to try to use neural networks to enhance the engine’s efficiency. So far I have tried two approaches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using a Neural Network as the Evaluation Function&lt;/strong&gt;&lt;br /&gt;
I mentioned this idea in the last post I made about the chess engine. As I mentioned in &lt;a href=&quot;https://mr4k.github.io/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html&quot;&gt;that post&lt;/a&gt; board game engines usually rely on two different components, search and evaluation. The search component looks ahead to try to predict the best move. However with complex games like chess we can not look at all the possible moves so eventually at some point during the search we need to start estimating how good certain far off positions are. The function which estimates how good a given state is for a player is called the evaluation function. This function takes a state (the chess board) as its input and returns a scalar estimate of how good the position is for one of the players. Intuituvely making a more accurate evaluation function will make the engine better as a whole. In state of the art chess engines, the evaluation functions are often large chunks of code, handwritten by expert chess players and tweaked by computers. Becuase I am abysmal at chess I cannot write such a function. But I though that I might be able to train a neural network to be a decent one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;br /&gt;
I downloaded a about a million games from the &lt;a href=&quot;http://www.ficsgames.org/download.html&quot;&gt;fics database&lt;/a&gt;. I then preprocessed them by seperating them into different board positions and counting the number times black(choosen arbitarily) wins, loses and ties after getting to that position. I ended up with about 600,000 unique positions that had been visited 3 times or more. I then calculated the percentage of times black won and trained a network to approximate that function. Unfortunetly it didn’t seem to work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What Went Wrong&lt;/strong&gt;&lt;br /&gt;
I think the data was a too noisy. The problem is that most games of human versus human chess can be turned around under perfect play so sometimes people should not lose when they do which adds noise to the data. I also used games between players of all ranks which means some of the games were probably just lost due to a low skill level not a bad board position. Finally the way I counted the positions was flawed because it did not take duplicates board positions into account. For example if one game had the same board configuration three times, it would count it three times as a win, loss or tie when it should only count it once.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using a Neural Network to Recommend Moves&lt;/strong&gt;&lt;br /&gt;
A couple days ago I decided to try a new approach to the problem. The idea behind this approach is simple. In some board positions there are certain moves that are really never worth making. For example if your Queen is about to be taken it very rarely makes sense to move a pawn on the other side of the board. These useless moves are obvious even to weak humans players (like myself) but usually a computer will search them to figure out that they aren’t good. This takes a lot of valuable seach time which could be used to look at the promising positions in more detail. So basically the question is can we build a function which returns a probability of any given move being the right move. Over the past few days I have been attempting to answer this question.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Network&lt;/strong&gt;&lt;br /&gt;
Below is a diagram of the network I used. Can you guess where the 4096 comes from?&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/network.png&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;The input layer consists of 516 nodes. This is because I use 8 nodes per square on the board (6 nodes to denote piece type + ownership 2 nodes) plus four extra nodes at the end to indicate castling rights. The numbers of nodes for the next few layers were basically decided arbitarily. The last layer represents every conceivable chess move. Any move in chess can be defined by a starting position and an ending position. There are 64 starting positions and 64 ending positions which produces 4096 possible moves in total. Now most of these moves are never possible once the rules are introduced but this structure gives us a nice way to enumerate any move. &lt;br /&gt;
All of these layers are dense and fully connected. But what about convolutional layers? Aren’t those great for image processing, a chessboard is like an image isn’t it? I decided not to use convolution for two reasons. The first was that I wanted to keep it simple at first and convolution seemed like overkill. The second was that I was worried that small convolutional filters might make the program too locally focused. For instance it might not understand that a bishop can move all the way across the board.&lt;br /&gt;
I also used both L2 regularizarion and dropout so that I would have to worry less about overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;br /&gt;
I used 200,000 games from &lt;a href=&quot;http://www.kingbase-chess.net/&quot;&gt;KingBase&lt;/a&gt;, a database of games between players ranked &amp;gt; 2000, to train the network. I took each board position and figured out the probability distributions over all the moves made from it during those games. Then I used that data to train the network for about 36 hours on my macbook (no gpu).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;br /&gt;
Before I write about these results I must make the disclaimer that my test set was a too small (only 61 board positions) so take them with a grain of salt. So the question is can it play chess? The answer is to an extent. Out of the 61 board positions, 56 of the networks top choices were legal moves. Not only were they legal, but some of them were recommended by hand tuned chess systems. Sometimes the network also made terrible moves. However these moves seemed to occur at the end of the game. I will post the code to build the data and train the network soon so anyone can try to replicate/analyze the results. I just need to clean it up a little first.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvement&lt;/strong&gt;&lt;br /&gt;
I will try to improve this system in a couple ways. First of all I will train it for longer on a gpu with more training data. I’m hoping this will help its early and middle game selections. Finally I will try to train the network using &lt;a href=&quot;http://www.scholarpedia.org/article/Policy_gradient_methods&quot;&gt;policy gradients&lt;/a&gt; or some similar method. This will allow the network to learn by palying itself and encounter a wider range of board positions that it does now. For example the network does not know how to play from a seriously disadvantaged position because pro games are usually pretty close.&lt;/p&gt;

</description>
        <pubDate>Sat, 23 Jul 2016 12:50:40 +0200</pubDate>
        <link>http://yourdomain.com/neural/network/deep/learning/chess/training/2016/07/23/deep-chess.html</link>
        <guid isPermaLink="true">http://yourdomain.com/neural/network/deep/learning/chess/training/2016/07/23/deep-chess.html</guid>
        
        
        <category>neural</category>
        
        <category>network</category>
        
        <category>deep</category>
        
        <category>learning</category>
        
        <category>chess</category>
        
        <category>training</category>
        
      </item>
    
      <item>
        <title>Getting Sentimental</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/t-wex.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;I just got back from a week long backpacking trip. I decided that I was going to take a break from my chess engine to do a couple quick experiments in other areas of machine learning. Yesterday I wrote a simple sentiment analyzer in 50 lines of python. In this post I will explain what I did and how it could be improved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Data&lt;/strong&gt;&lt;br /&gt;
I got my data from the &lt;a href=&quot;http://help.sentiment140.com/for-students/&quot;&gt;Sentiment 140&lt;/a&gt; database. It contains 1.6 million tweets labeled as either positive negative or neutral. I only used the positive and negative tweets from the database because I was using a binary classifier for simplicity. As a side note databases like Sentiment 140 can be quite biased towards the opinions of certain groups of people. I found &lt;a href=&quot;https://www.oreilly.com/learning/how-we-amplify-privilege-with-supervised-machine-learning&quot;&gt;this talk&lt;/a&gt; to be very interesting and intutive explantion of the biases in supervised machine learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Algorithm&lt;/strong&gt;&lt;br /&gt;
The algorithm I used is pretty basic as natural language processing goes. It has two main steps, preproccesing the text and classification.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;&lt;br /&gt;
First I convert the tweets into vectors using a bigram model. This means that each unique word and pair of words becomes its own feature.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#This tweet&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;I am a drama llama&#39;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Produces these features&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;I&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;am&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;drama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;llama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;I am&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;am a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;a drama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;drama llama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Our feature space is contructed by feeding all of our training data into this model. To convert a tweet to a vector all we do is make a vector which counts how many times each feature occurs in the tweet multiplied by two scalar terms called the term frequency and the inverse document frequency (idf).&lt;br /&gt;
The term frequency of a feature is simply the frequency of the feature in the current document(tweet). The rational behind the tf is that words that occur more might matter more. The inverse document frequency is defined as follows,&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/idf.png&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;where N is the total number of documents and &lt;em&gt;df&lt;/em&gt; is the number of documents containing our feature. As far as I can tell the &lt;em&gt;log&lt;/em&gt; is only there to keep the size of the idf from exploding. The rational behind this term is that features that occur in multiple documents are more likely to provide important clues about the document so they should be weighed higher. “But wait,”, you ask, “what about word like ‘the’, they occur everywhere but are essentially meaningless.” Well the answer is that we ignore the words that occur in too many documents and too few documents. In this way wors like ‘the’ will be ignored because they are too frequent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;br /&gt;
Now that we have high dimensional representions for the tweets as vectors we just need a way to map those vectors to a sentiment (good or bad in this case). I’ve chosen to a support vector machine to classify the tweets. A support vector machine just finds the hyperplane which best divides the two labeled sets of tweet vectors (good and bad). Then we classify new tweets as good or bad based on which side of the plane they are on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;br /&gt;
Below is the full source code. Feel free to run it yourself. You’ll need the sentiment140 data to do so as well as numpy, sci-py and, scikit-learn.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#the parameters min_df and max_df where choosen experimentally&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#the hinge parameter specfices that we are using an svm&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;tfidf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngram_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;ISO-8859-1&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.002&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
		&lt;span class=&quot;n&quot;&gt;min_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00015&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;clf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hinge&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;l2&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data/training.1600000.processed.noemoticon.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data/testdata.manual.2009.06.14.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;#In the data there are three classes of data [0:negative, 2:neutral, 4:positive] &lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#For this example we are only going to tag things are positive or negative&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loading Data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Constructing Model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Predicting Output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Enter Some Phrases to Test the Sentiment Analyzer or Type &#39;quit&#39; to Exit.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;raw_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;gt;&amp;gt;&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;quit&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;class:&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Improvements&lt;/strong&gt;&lt;br /&gt;
This solution is only 80% accurate on the test set. That is a significant improvement on the 50% chance of guessing randomly but there is still a long way to go. Possible improvements could include, taking word order into account, uzing something like &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;word2vec&lt;/a&gt; to understand the meanings of words better, using a more complex classifier than a svm.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jul 2016 11:40:40 +0200</pubDate>
        <link>http://yourdomain.com/sentiment/analysis/logistic/regression/n-gram/2016/07/14/sentiment.html</link>
        <guid isPermaLink="true">http://yourdomain.com/sentiment/analysis/logistic/regression/n-gram/2016/07/14/sentiment.html</guid>
        
        
        <category>sentiment</category>
        
        <category>analysis</category>
        
        <category>logistic</category>
        
        <category>regression</category>
        
        <category>n-gram</category>
        
      </item>
    
      <item>
        <title>The Count of Monte Carlo</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/tree_comic.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;A couple days ago started building a small chess engine. This weekend I decided to give it a web interface. I ended up using &lt;a href=&quot;http://flask.pocoo.org/&quot;&gt;Flask&lt;/a&gt; for the backend because I like python. You can play a slighly less intelligent version (due to memory constraints on server) &lt;a href=&quot;http://playchess-pstefek.rhcloud.com/&quot;&gt;here&lt;/a&gt;. You can also view the source code on &lt;a href=&quot;https://github.com/Mr4k/playchess&quot;&gt;github&lt;/a&gt; but be warned, I have not cleaned it up yet. Although I have a working application I’m far from done. The algorithm does not play very well right now. The moves it generates make some sense but it usually slips up if you pressure it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Current Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current algorithm I am using is pretty simple. There are two main components, search and evaluation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Search Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Right now I am using a search algorithm called minimax along with a technique called alpha beta pruning. The basic principle behind 2 player minimax is that the best move is the one which minimizes your opponent’s potential gain. Before we look at how this applies to chess let’s look at a very simple example.&lt;/p&gt;

&lt;p&gt;In traditional math style, Alice and Bob are bored at home and decided to play chess. Because Alice is a chess prodigy, she destroys Bob five to nothing. To make him feel better, Alice challenges him to a new game called Trenchwaffle which heavily favors Bob. In Trenchwaffle each player can either dig a trench (branch right) or eat a waffle (branch left). There are only three turns in the whole game (Bob moves, then Alice moves, then Bob moves once more). Although the rules of Trenchwaffle are vague and mysterious we manage to procure a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Game_tree&quot;&gt;game tree&lt;/a&gt;. So the question is should Bob eat a waffle (branch left) or dig a trench (branch right)? The following diagram shows how to use the minimax algorithm to figure out which move Bob should make:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/blog/trenchwaffle.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;According to minimax, Bob should eat a waffle (branch left) because he will always win no matter what Alice chooses to do. Because Trenchwaffle is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Solved_game&quot;&gt;solvable&lt;/a&gt; minimax will find a winning strategy for Bob if one exists. So why does this not work with chess, isn’t &lt;a href=&quot;https://en.wikipedia.org/wiki/Solving_chess&quot;&gt;chess solvable&lt;/a&gt;? The answer is that the game tree for chess is far too big to compute in full. The average &lt;a href=&quot;https://en.wikipedia.org/wiki/Branching_factor&quot;&gt;branching factor&lt;/a&gt; for chess is around 36. So the sixth layer of the game tree contains around 36&lt;sup&gt;6&lt;/sup&gt; moves. The tenth layer contains around 36&lt;sup&gt;10&lt;/sup&gt; moves.&lt;/p&gt;

&lt;p&gt;To apply minimax to a game like chess there are too many moves to look at we only look a small number of moves ahead and then guess how good each leaf node of our game tree is. In my current implementation I look 5 half moves ahead and even then I still need to reduce the number of nodes searched using &lt;a href=&quot;https://www.youtube.com/watch?v=Ewh-rF7KSEg&quot;&gt;alpha beta pruning&lt;/a&gt;. So now we just need a way to determine how good a position is for a particular player. This is called the evaluation function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Evaluation Function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The evaluation function takes a game state as a parameter and returns a number which represents how good that state is for one of the two players. My evaluation function takes a chess board and returns how good it thinks black is doing (larger is better).&lt;/p&gt;

&lt;p&gt;The current evalution function I am using looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function evaluate(board)
  return (total value of pieces black captured) - (total value of pieces white captured)
   + 0.1 * (total value of pieces black is attacking) + 0.1 * (total value of pieces black is defending)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It’s defintely not great and I am sure someone who was good at chess could write me a better one. However I have some other ideas to improve the evaluation function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Future&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While I think I will hold onto the search and evaluate model, I think there are a number of ways to improve both components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improving Search&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The draw back to minimax in any form is that it is slow. By that I mean it takes about 2 minutes to search 5 half moves ahead even with alpha beta pruning. This also means it can’t look super far ahead to set up any mind blowing traps. There are several options I have to improve the search. 1. Figure out how to better prune the branches. One way to do this might be to put nodes through an evaluation function to see if they are worth exploring. 2. Use a different algorithm. The Monte Carlo Search Tree Algorithm (for which this post is named) looks promising.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improving the evaluation function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is where I might try machine learning. Chess is a nuanced game. A well placed knight could be worth losing a rook maybe even a queen but it’s usually not. I’m hoping that thousands of previous chess games describe these situations better than I can.&lt;/p&gt;

</description>
        <pubDate>Tue, 28 Jun 2016 22:30:40 +0200</pubDate>
        <link>http://yourdomain.com/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html</link>
        <guid isPermaLink="true">http://yourdomain.com/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html</guid>
        
        
        <category>chess</category>
        
        <category>project</category>
        
        <category>minimax</category>
        
        <category>algorithm</category>
        
        <category>search</category>
        
        <category>monte</category>
        
        <category>carlo</category>
        
      </item>
    
  </channel>
</rss>
