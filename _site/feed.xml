<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peter Stefek</title>
    <description>I&#39;m a rising third year student at Oberlin College. I am currently majoring in Math and Computer Science. 
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 27 Jul 2017 18:53:30 -0700</pubDate>
    <lastBuildDate>Thu, 27 Jul 2017 18:53:30 -0700</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Do I Feel Lucky?</title>
        <description>&lt;style&gt;
thick.thicker {
    font-weight: 900;
}
&lt;/style&gt;

&lt;p&gt;&lt;strong&gt;&lt;text class=&quot;thick&quot;&gt;Lets say you have a bag with 2 orange marbles, 2 blue marbles and 2 green marbles&lt;/text&gt;.&lt;/strong&gt; You draw one marble out of the bag. The probability of drawing any color marble is 1/3. This is called a uniform discrete distribution. It’s easy to draw from this distribution with a computer. Computers can generate uniform pseudo random numbers in constant time so all you would have to do is generate a number from 1 to 3 and map it to a color of marble depending on the result. This can be done in constant time for any arbitrary number of colors as long as there are the same number of each color of marble.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;But now let’s try a slightly different problem.&lt;/strong&gt; Let’s take a bag with 1 orange marble, 2 blue marbles and 4 green marbles. This distribution is a general discrete distribution. Clearly we can’t tackle this exactly the same way as before. One simple adjustment we could make is to generate a random number between 1 and 7 and map it to 7 boxes. We could put a marble in each box and assign a orange marble to one box, blue marbles to two boxes and green marbles to four boxes. We then lookup the box which corresponds to the number we generate and check which kind of marble it has. This generates the correct distribution.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Finally now let’s look at a different general discrete distribution.&lt;/strong&gt; For example, the distribution of orange, blue and green marbles with weights: p(orange) = 251/1000, p(blue) = 479/1000 and p(green) = 270/1000. We cannot use the first technique because there is no easy way to simplify these numbers. The second technique would require us to create 1000 boxes, so it also seems pretty bad. The weakness of our second technique is that the setup time depends on the weights, so the messier the numbers are the longer (and more space) it takes. This is not ideal. &lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Now we can talk about a simple algorithm which solves our problem.&lt;/strong&gt; The plan is to partition the interval [0,1] into n different parts based on the probabilities of each event. In our above example we could partition the interval into three sections with sizes orange = 0.251, blue = 0.479 and green = 0.27. Below is an O(n) algorithm to do this.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weightedRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	Draw from a general discrete distribution.
	:param weights: A dictionary of weights which sum to one.
	:return: A random sample from it the distribution defined by the weights.
	&quot;&quot;&quot;&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;#generate a uniform random number from 0 - 1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;remainder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;remainder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remainder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;orange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;green&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;orange&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;green&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;600000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weightedRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;So can we do better?&lt;/strong&gt; The answer is sort of. It depends on what we want to do. It turns out if we want to sample from the same distribution multiple times, then we can. We will do just one preprocessing step (which will be the same time/space complexity for any set of n weights) and after that each sample will be O(1). So the more times we want to sample from the same distribution, the better the performance will be.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;How does this work?&lt;/strong&gt;
I’m going to go over the preprocessing step in a later section but first I’m going to describe how we do the O(1) lookups. Consider a general discrete probability distribution with n weights. We are going to divide it up into n boxes such that each box will contain pieces of either one or two different different weights and the pieces of these weights will sum to 1/n. From our definition the sum of all the pieces of all the weights in the boxes is 1 (the size of the entire distribution).&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;The description can be difficult.&lt;/strong&gt; I think a picture can really help. In this example we have three colored balls with weights p(blue) = 1/6, p(orange) = 3/6, p(green) = 2/6:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/alias_diag.png&quot; width=&quot;56%&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once we have this partitioning, our algorithm is very simple:&lt;/strong&gt; &lt;br /&gt;
First we pick one of the 3 boxes. Then we choose a uniform random number between 0 and 1. We see which side of our chosen box it falls on and we return that color marble.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Now we need to find these partitions.&lt;/strong&gt; First of you might be asking yourself, why can we always partition the distribution like this? To answer that we will first provide an algorithm to construct these partitions then prove it always works.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;The basic idea of this algorithm is about filling up these boxes.&lt;/strong&gt; First a quick observation. If we look at all the weights in a normalized (the sum of all the weights is one) probability distribution at least one of them must be less than or equal to 1/n where n is the total number of weights. Why is this important? This means that we can always fit one of these weights into a box of size 1/n possibly with some left over space.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Our partitioning algorithm is as follows:&lt;/strong&gt;&lt;br /&gt;
1) Sort the weights from least to greatest.&lt;br /&gt;
2) Choose the smallest weight and put it into a box. Then if there is any space left over, fill in the extra room with some of the largest weight.&lt;br /&gt;
3) Repeat steps 1 and 2 until all of the boxes have been filled.&lt;br /&gt;
Here’s a visualization of the algorithm:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/alias_anim.gif&quot; width=&quot;56%&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now why can we always do this?&lt;/strong&gt; Now that we have the algorithm we can use induction to give a formal proof. Our claim is a little more general from what we have been saying earlier but it will make the proof easier.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Claim:&lt;/strong&gt; Given a set of n non-zero real numbers (called weights) we can partition them into n boxes of size s/n (where s is the sum of all n weights) such that each box only contains pieces of at most 2 weights using our algorithm.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;For the base case:&lt;/strong&gt; Clearly if we have just one weight we can just put it in a single box by itself.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Now comes our inductive step:&lt;/strong&gt; Suppose that we know our claim is true for n - 1 weights. Now we must show it is true for n weights.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Let’s do the first step of our algorithm:&lt;/strong&gt; We put the smallest weight in a box of size s/n. We know that it must fit because if it did not our weights would sum to more than s. Now if there is any left over space we fill it with some of our largest weight (by similar logic to the previous sentence this is also always possible). Finally we are left with one filled box and n - 1 unfilled boxes. We know by the inductive hypothesis (our assumption) that we can split whatever is remaining into those n - 1 boxes. Therefore we are done.&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Now finally what is the running time of the preprocessing step?&lt;/strong&gt; First we need to sort all the weights which is O(nlog n). Now we need to do n steps where we get rid of the smallest weight and restart. However the key here is that only the largest weight needs to considered for sorting because its the only value let in the partition that is changed besides the one removed. We can do this in log n time with a binary search (because we have already sorted our weights). Therefore the rest of the algorithm is also O(n log n) so the whole algorithm is O(n log n).&lt;br /&gt;
&lt;br /&gt;
&lt;strong&gt;Here is an implementation of our algorithm:&lt;/strong&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bintrees&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AVLTree&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;partitionWeights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	The preprocessing step.
	:param weights: A dictionary of weights which sum to one.
	:return: A partition used to draw quickly from the distribution
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00001&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# for floating point precision issues&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;numWeights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;# We use a AVLTree to make our pull/push operations O(log n)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AVLTree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numWeights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;smallestValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smallestColor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# O(log n)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;overfill&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numWeights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smallestValue&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overfill&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;largestValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;largestColor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# O(log n)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;largestValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overfill&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;largestValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;largestValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;largestColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# O(log n)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smallestValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smallestColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;largestColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smallestValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smallestColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;none&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;drawFromPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	The draw step.
	:param partition: partition A partition of a distribution into boxes.
	:return: A sample from the distribution represented by the partition.
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;numBoxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numBoxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numBoxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color1&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;orange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;green&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitionWeights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;orange&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;green&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;600000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drawFromPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;One final note on runtime.&lt;/strong&gt; If you run our example code you’ll notice the “slow” function is almost twice as fast as the “fast” one. Have I been lying this whole time? No. This is because in our example we used a small number of weights. In a &lt;a href=&quot;https://gist.github.com/Mr4k/eabaca318499bd54e5e18431efbc6622&quot;&gt;separate speed test&lt;/a&gt; I use 1000 weights and draw from the distributions 100000 times each. In this case the fast algorithm runs in 0.35 seconds on my computer while the slow algorithm takes about 15 seconds.&lt;/p&gt;
</description>
        <pubDate>Sun, 25 Jun 2017 04:00:40 -0700</pubDate>
        <link>http://yourdomain.com/brain/teaser/algorithm/probability/alias/2017/06/25/alias.html</link>
        <guid isPermaLink="true">http://yourdomain.com/brain/teaser/algorithm/probability/alias/2017/06/25/alias.html</guid>
        
        
        <category>brain</category>
        
        <category>teaser</category>
        
        <category>algorithm</category>
        
        <category>probability</category>
        
        <category>alias</category>
        
      </item>
    
      <item>
        <title>Mind Sweeper</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/minesweeper.jpg&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;This weekend I braved the cold and visted my brother in upstate New York. While I was there he showed me minesweeper and told me he hadn’t beaten it yet. Although I had heard of minesweeper I had never played it so I gave it a couple (many) tries but I just did not have the patience to win. Several failed attempts later I decided that it might be easier to write a &lt;a href=&quot;https://github.com/Mr4k/sweeper&quot;&gt;program&lt;/a&gt; which used my general strategy but had an unlimited attention span rather than continuing to accidently click bombs I meant to flag. Over the next couple hours I was able to write a simple program which beat the game in a couple of tries. Below is a gif of the program playing minesweeper (located &lt;a href=&quot;http://minesweeperonline.com/&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/minesweeper-play.gif&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;The program can be broken down into three main parts; translating the pixels on the screen into an internal representation of the board (vision), picking the next best move (general strategy), and moving the cursor to click on the chosen tile (input).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vision&lt;/strong&gt;&lt;br /&gt;
In order for our program to be able to play minesweeper, it first must be able to understand what’s on the screen. First we take a screenshot of the play area. Then we need to translate those pixels into a 2d array representing the board. Luckily we don’t need fancy computer vision algorithms here. It turns out we need to do is check the color of the center of each tile and compare it to the colors of each of the numbers. We can march along the board tile by tile and check which number is in each one. To compare two colors we can just take the Euclidean distance and check if it is under a certain threshold.&lt;br /&gt;
The only catch is that it turns out that the empty tile is the same color as the un checked tile. This means that we don’t know if a tile has zero mines around it or an unknown number of mines. What I ended up doing was flood filling the board with zeros starting from the tile that was last clicked. This is similar to how minesweeper decides how many tiles to unravel so it produces an accurate picture of the board.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;General Strategy&lt;/strong&gt;&lt;br /&gt;
After we process the screen we must decide where best to click. Luckily minesweeper is not GO so we don’t need any fancy machine learning methods. Instead I did the following.&lt;br /&gt;
First I labeled all the tiles we know to be bombs. To do this we check for each tile if the amount of unknown bombs surrounding it equal to the amount of unknown tiles around it. If they are we know all of those tiles are bombs and can label them accordingly on our board. Then we recursively perform the same check on all the explored neighbors of those tiles. After that we look at each tile and see if there are any unexplored tiles that we can be certain are not bombs. Using these methods we can solve large chunks of the minesweeper board.&lt;br /&gt;
However eventually we will run out of information and we must guess. As far as I know there is no perfect way to guess. I decided to use a simple system in which we just pick a tile around the explored tile which has the lowest ratio of unknow tiles to bombs around it. I am sure that this heurtistic could be improved and would probably dramatically increase the win rate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;br /&gt;
Finally after we have chosen our next move we need to click the position. For this task I used &lt;a href=&quot;https://pypi.python.org/pypi/PyAutoGUI&quot;&gt;pyautogui&lt;/a&gt; which makes gui automation simple.&lt;/p&gt;

&lt;p&gt;If you would like more detail you should check out the &lt;a href=&quot;https://github.com/Mr4k/sweeper&quot;&gt;source&lt;/a&gt; on github.&lt;/p&gt;

</description>
        <pubDate>Sun, 29 Jan 2017 02:50:40 -0800</pubDate>
        <link>http://yourdomain.com/mindsweeper/video/game/algorithm/python/2017/01/29/minesweeper-copy.html</link>
        <guid isPermaLink="true">http://yourdomain.com/mindsweeper/video/game/algorithm/python/2017/01/29/minesweeper-copy.html</guid>
        
        
        <category>mindsweeper</category>
        
        <category>video</category>
        
        <category>game</category>
        
        <category>algorithm</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Brain Pain</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/riddle.jpg&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;This semester I am studying abroad at the &lt;a href=&quot;http://www.budapestsemesters.com/&quot;&gt;Budapest Semesters in Mathamatics&lt;/a&gt; program. This means that I am always surrounded by people who enjoy solving puzzles also known as mathmaticans. One of my roommates gave me this problem and I thought that it was worth sharing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/rect-problem.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Using only a straight edge and a pencil construct a line which divides the shaded area in half. The straight edge has no markings so you cannot measure anything.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;View the solution&lt;/summary&gt;
  The key insight is that any line which passes through the center of a rectangle divides it in half. We can think of the area of the big rectangle as positive and the area of the little rectangle as negative. The area of the shaded region is the sum of the positive and negative areas. So if we can make a line which divides both rectangles in half we will have equal amounts of positive and negative area on both sides of the line.      

  &lt;p&gt;First we construct the center of one of the rectangles. We can do this by finding the intersection of the diagonals.&lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/rect-centers-1.png&quot; /&gt; 
  &lt;/p&gt;  
  Then we construct the center of the other rectangle.
  &lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/rect-centers-2.png&quot; /&gt; 
  &lt;/p&gt;  
  Finally we constuct the dividing line so that it goes through both centers.
  &lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/rect-dividing-line.png&quot; /&gt; 
  &lt;/p&gt;  
  Note that this solution works for any orientation/position/scale of the rectangles.
&lt;/details&gt;

</description>
        <pubDate>Fri, 09 Sep 2016 03:50:40 -0700</pubDate>
        <link>http://yourdomain.com/brain/teaser/puzzle/construction/2016/09/09/brain-pain.html</link>
        <guid isPermaLink="true">http://yourdomain.com/brain/teaser/puzzle/construction/2016/09/09/brain-pain.html</guid>
        
        
        <category>brain</category>
        
        <category>teaser</category>
        
        <category>puzzle</category>
        
        <category>construction</category>
        
      </item>
    
      <item>
        <title>Out of My Depth (Part 2)</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/matrix_transform.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;In the next part in this series of articles about depth recontruction we will talk about the essential matrix. Before we define it we have to look back at another mathamatical idea.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Cross Product&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/cross-prod.png&quot; height=&quot;300&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;As many readers will know, the cross product takes two vectors and cretes a vector perpendicular to both of them. It is important to note that if the two vectors are not independent (that is if one vector is a scalar multiple of the other) then the cross product is the zero vector. What does the cross product actually look like?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/cross-equation.gif&quot; height=&quot;75&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Truthfully they’re are better ways to think of the cross product but this one is useful because it allows us to see exactly what the end result of the cross product is. This brings us to the next concept. For any vector a, we can make a matrix C(a) such that for any vector b, a x b = C(a)b.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/cross-matrix.gif&quot; height=&quot;75&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;So why is this useful? Well remember the equation from the end of the &lt;a href=&quot;% post_url 2016-08-21-sfm2 %&quot;&gt;last post&lt;/a&gt;?&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;Rp&lt;sub&gt;2&lt;/sub&gt; + T
&lt;/p&gt;

&lt;p&gt;Well now what if we take C(T) and multiple by it on the right.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;C(T)p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;C(T)Rp&lt;sub&gt;2&lt;/sub&gt; + C(T)T
&lt;/p&gt;

&lt;p&gt;Notice that C(T)T is the zero vector because you are crossing T with itself.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;C(T)p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;C(T)Rp&lt;sub&gt;2&lt;/sub&gt;
&lt;/p&gt;

&lt;p&gt;Now we get a little more clever and multiply both sides on the right by p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;C(T)p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;C(T)Rp&lt;sub&gt;2&lt;/sub&gt;
&lt;/p&gt;

&lt;p&gt;Now we know that p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;C(T)p&lt;sub&gt;1&lt;/sub&gt; = 0 because C(T)p&lt;sub&gt;1&lt;/sub&gt; is perpendicular to p&lt;sub&gt;1&lt;/sub&gt; and multiplication by p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt; is equivalent to a dot product. So our equation becomes:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	0 = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;C(T)Rp&lt;sub&gt;2&lt;/sub&gt;
&lt;/p&gt;

&lt;p&gt;We can divide by λ&lt;sub&gt;2&lt;/sub&gt; to get:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	0 = p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;C(T)Rp&lt;sub&gt;2&lt;/sub&gt;
&lt;/p&gt;

&lt;p&gt;Finally we will define the essential matrix:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	E := C(T)R
&lt;/p&gt;

&lt;p&gt;We can rewrite the equation above in terms of this new definition.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
	0 = p&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;Ep&lt;sub&gt;2&lt;/sub&gt;
&lt;/p&gt;

&lt;p&gt;So why did we go through all this work just to get this new matrix? Well most importantly E provides a linear relation between p&lt;sub&gt;1&lt;/sub&gt; and p&lt;sub&gt;2&lt;/sub&gt;. This means we can solve for E easily using a standard system of equations. So now all we need is a way to get R and T out of E and we can solve for the depth(coming in Part 3).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Significance of the Essential Matrix&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Is E just a meaningless intermediate stepin our problem. Not exactly. Lets consider what E is. E maps points from the screen space of camera 1 to the screen space of camera 2. Basically if you have E project any point in camera 1 in camera 2 which could have all sorts of cool applications. For example overlaying two images on top of one another. Or drawing on one image and overlaying your drawing onto an entire video (obviously if the angle varies too much there could be depth issues).&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Aug 2016 05:01:00 -0700</pubDate>
        <link>http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/20/sfm.html</link>
        <guid isPermaLink="true">http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/20/sfm.html</guid>
        
        
        <category>structure</category>
        
        <category>from</category>
        
        <category>motion</category>
        
        <category>depth</category>
        
        <category>linear</category>
        
        <category>algebra</category>
        
      </item>
    
      <item>
        <title>Out of My Depth (Part 1)</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/headache.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;In computer vision the structure from motion problem is centered around reconstructing a 3d scene from a set of two dimensional images. I’ve always found this idea to be really cool. The app &lt;a href=&quot;http://www.123dapp.com/catch&quot;&gt;123d Catch&lt;/a&gt; by Autodesk is great for playing around with these ideas, allowing you to build 3D models by taking pictures of an object on your phone.&lt;/p&gt;

&lt;p&gt;There are two distinct parts of the structure from motion problem.&lt;/p&gt;

&lt;p&gt;The first part is called feature mapping. Given a set of two dimensional images, the first thing we need to do is identify which parts of one image correspond to parts of the others. This article will not address feature mapping but if you are interested in how it works I would recommend looking at the SIFT or FAST algorithms.&lt;/p&gt;

&lt;p&gt;In this series of articles we will address the second part of the structure from motion problem. Suppose we have two images, I&lt;sub&gt;1&lt;/sub&gt; and I&lt;sub&gt;2&lt;/sub&gt; . We also have some points p&lt;sub&gt;1&lt;/sub&gt; in I&lt;sub&gt;1&lt;/sub&gt; which correspond to the points p&lt;sub&gt;2&lt;/sub&gt; in I&lt;sub&gt;2&lt;/sub&gt;. How can we find the depth of each point?&lt;/p&gt;

&lt;p&gt;To begin we have to talk about cameras. The simplest camera is called a pinhole camera. You’ve probably seen one before in elementary school science class.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/pinhole.gif&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Pinhole Camera Model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For this algorithm we will assume the camera through which I&lt;sub&gt;1&lt;/sub&gt; and I&lt;sub&gt;2&lt;/sub&gt; were taken is a perfect pinhole camera. This means that the pinhole is really small. We will refer to the back of the camera as the image plane. Because the pinhole is infinitesimally small we know that for any 2d point on the image plane (p&lt;sub&gt;x&lt;/sub&gt;, p&lt;sub&gt;y&lt;/sub&gt;) the corresponding 3d point, (P&lt;sub&gt;x&lt;/sub&gt;, P&lt;sub&gt;y&lt;/sub&gt;, P&lt;sub&gt;z&lt;/sub&gt;), must lie along on a ray which starts at the origin with direction (-p&lt;sub&gt;x&lt;/sub&gt;, -p&lt;sub&gt;y&lt;/sub&gt;, 1). Note that we assume the pinhole is the origin of our the 3d coordinate system. Now we will define the depth of a particular point (λ) as the distance from that point to the origin. All of this is summarized by the following equation (note that we drop a negative sign for simplicity so the sign of λ will flip):&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;(p&lt;sub&gt;x&lt;/sub&gt;, p&lt;sub&gt;y&lt;/sub&gt;, 1) = (P&lt;sub&gt;x&lt;/sub&gt;, P&lt;sub&gt;y&lt;/sub&gt;, P&lt;sub&gt;z&lt;/sub&gt;)
&lt;/p&gt;
&lt;p&gt;In vector notation this becomes:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;p = P
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Two Camera Model&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/twocamera.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Let’s assume we have two perfect pinhole cameras C&lt;sub&gt;1&lt;/sub&gt;, C&lt;sub&gt;2&lt;/sub&gt; each defined by a point in space and the direction it faces. Now for simplicity center the coordinate system around C&lt;sub&gt;1&lt;/sub&gt;, such that it’s center is at the origin and it points straight down the z-axis. Now we know that we there exists an affline transformation (a rotation R and a translation T) which will center the coordinate system around C&lt;sub&gt;2&lt;/sub&gt;. So for any point p in the image plane of C&lt;sub&gt;1&lt;/sub&gt; we can relate it to its corresponding point in the image plane of C&lt;sub&gt;2&lt;/sub&gt; with the equation:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 
	&amp;lambda;&lt;sub&gt;1&lt;/sub&gt;p&lt;sub&gt;1&lt;/sub&gt; = &amp;lambda;&lt;sub&gt;2&lt;/sub&gt;Rp&lt;sub&gt;2&lt;/sub&gt; + T
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Complications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So you might be thinking that if we had enough points we could set up a system of equations and solve for the unknown variables above. The problem with this approach is that the above equation is not linear because λ&lt;sub&gt;2&lt;/sub&gt;Rp&lt;sub&gt;2&lt;/sub&gt; contains the product of two unknowns (R and λ&lt;sub&gt;2&lt;/sub&gt;). This makes it very difficult to solve. However in the next article we will use some clever math turn this into a system we can solve.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Aug 2016 04:00:50 -0700</pubDate>
        <link>http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/11/sfm2.html</link>
        <guid isPermaLink="true">http://yourdomain.com/structure/from/motion/depth/linear/algebra/2016/08/11/sfm2.html</guid>
        
        
        <category>structure</category>
        
        <category>from</category>
        
        <category>motion</category>
        
        <category>depth</category>
        
        <category>linear</category>
        
        <category>algebra</category>
        
      </item>
    
      <item>
        <title>Regress the Chess</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/training.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;Despite the lack of posts recently I have not given up on the chess engine but it will definitely take longer than I thought. Recently I decide to try to use neural networks to enhance the engine’s efficiency. So far I have tried two approaches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using a Neural Network as the Evaluation Function&lt;/strong&gt;&lt;br /&gt;
I mentioned this idea in the last post I made about the chess engine. As I mentioned in &lt;a href=&quot;https://mr4k.github.io/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html&quot;&gt;that post&lt;/a&gt; board game engines usually rely on two different components, search and evaluation. The search component looks ahead to try to predict the best move. However with complex games like chess we can not look at all the possible moves so eventually at some point during the search we need to start estimating how good certain far off positions are. The function which estimates how good a given state is for a player is called the evaluation function. This function takes a state (the chess board) as its input and returns a scalar estimate of how good the position is for one of the players. Intuituvely making a more accurate evaluation function will make the engine better as a whole. In state of the art chess engines, the evaluation functions are often large chunks of code, handwritten by expert chess players and tweaked by computers. Becuase I am abysmal at chess I cannot write such a function. But I though that I might be able to train a neural network to be a decent one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;br /&gt;
I downloaded a about a million games from the &lt;a href=&quot;http://www.ficsgames.org/download.html&quot;&gt;fics database&lt;/a&gt;. I then preprocessed them by seperating them into different board positions and counting the number times black(choosen arbitarily) wins, loses and ties after getting to that position. I ended up with about 600,000 unique positions that had been visited 3 times or more. I then calculated the percentage of times black won and trained a network to approximate that function. Unfortunetly it didn’t seem to work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What Went Wrong&lt;/strong&gt;&lt;br /&gt;
I think the data was a too noisy. The problem is that most games of human versus human chess can be turned around under perfect play so sometimes people should not lose when they do which adds noise to the data. I also used games between players of all ranks which means some of the games were probably just lost due to a low skill level not a bad board position. Finally the way I counted the positions was flawed because it did not take duplicates board positions into account. For example if one game had the same board configuration three times, it would count it three times as a win, loss or tie when it should only count it once.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using a Neural Network to Recommend Moves&lt;/strong&gt;&lt;br /&gt;
A couple days ago I decided to try a new approach to the problem. The idea behind this approach is simple. In some board positions there are certain moves that are really never worth making. For example if your Queen is about to be taken it very rarely makes sense to move a pawn on the other side of the board. These useless moves are obvious even to weak humans players (like myself) but usually a computer will search them to figure out that they aren’t good. This takes a lot of valuable seach time which could be used to look at the promising positions in more detail. So basically the question is can we build a function which returns a probability of any given move being the right move. Over the past few days I have been attempting to answer this question.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Network&lt;/strong&gt;&lt;br /&gt;
Below is a diagram of the network I used. Can you guess where the 4096 comes from?&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/network.png&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;The input layer consists of 516 nodes. This is because I use 8 nodes per square on the board (6 nodes to denote piece type + ownership 2 nodes) plus four extra nodes at the end to indicate castling rights. The numbers of nodes for the next few layers were basically decided arbitarily. The last layer represents every conceivable chess move. Any move in chess can be defined by a starting position and an ending position. There are 64 starting positions and 64 ending positions which produces 4096 possible moves in total. Now most of these moves are never possible once the rules are introduced but this structure gives us a nice way to enumerate any move. &lt;br /&gt;
All of these layers are dense and fully connected. But what about convolutional layers? Aren’t those great for image processing, a chessboard is like an image isn’t it? I decided not to use convolution for two reasons. The first was that I wanted to keep it simple at first and convolution seemed like overkill. The second was that I was worried that small convolutional filters might make the program too locally focused. For instance it might not understand that a bishop can move all the way across the board.&lt;br /&gt;
I also used both L2 regularizarion and dropout so that I would have to worry less about overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;br /&gt;
I used 200,000 games from &lt;a href=&quot;http://www.kingbase-chess.net/&quot;&gt;KingBase&lt;/a&gt;, a database of games between players ranked &amp;gt; 2000, to train the network. I took each board position and figured out the probability distributions over all the moves made from it during those games. Then I used that data to train the network for about 36 hours on my macbook (no gpu).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;br /&gt;
Before I write about these results I must make the disclaimer that my test set was a too small (only 61 board positions) so take them with a grain of salt. So the question is can it play chess? The answer is to an extent. Out of the 61 board positions, 56 of the networks top choices were legal moves. Not only were they legal, but some of them were recommended by hand tuned chess systems. Sometimes the network also made terrible moves. However these moves seemed to occur at the end of the game.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvement&lt;/strong&gt;&lt;br /&gt;
I will try to improve this system in a couple ways. First of all I will train it for longer on a gpu with more training data. I’m hoping this will help its early and middle game selections. Finally I will try to train the network using &lt;a href=&quot;http://www.scholarpedia.org/article/Policy_gradient_methods&quot;&gt;policy gradients&lt;/a&gt; or some similar method. This will allow the network to learn by palying itself and encounter a wider range of board positions that it does now. For example the network does not know how to play from a seriously disadvantaged position because pro games are usually pretty close.&lt;/p&gt;

</description>
        <pubDate>Sat, 23 Jul 2016 03:50:40 -0700</pubDate>
        <link>http://yourdomain.com/neural/network/deep/learning/chess/training/2016/07/23/deep-chess.html</link>
        <guid isPermaLink="true">http://yourdomain.com/neural/network/deep/learning/chess/training/2016/07/23/deep-chess.html</guid>
        
        
        <category>neural</category>
        
        <category>network</category>
        
        <category>deep</category>
        
        <category>learning</category>
        
        <category>chess</category>
        
        <category>training</category>
        
      </item>
    
      <item>
        <title>Getting Sentimental</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/t-wex.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;I just got back from a week long backpacking trip. I decided that I was going to take a break from my chess engine to do a couple quick experiments in other areas of machine learning. Yesterday I wrote a simple sentiment analyzer in 50 lines of python. In this post I will explain what I did and how it could be improved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Data&lt;/strong&gt;&lt;br /&gt;
I got my data from the &lt;a href=&quot;http://help.sentiment140.com/for-students/&quot;&gt;Sentiment 140&lt;/a&gt; database. It contains 1.6 million tweets labeled as either positive negative or neutral. I only used the positive and negative tweets from the database because I was using a binary classifier for simplicity. As a side note databases like Sentiment 140 can be quite biased towards the opinions of certain groups of people. I found &lt;a href=&quot;https://www.oreilly.com/learning/how-we-amplify-privilege-with-supervised-machine-learning&quot;&gt;this talk&lt;/a&gt; to be very interesting and intutive explantion of the biases in supervised machine learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Algorithm&lt;/strong&gt;&lt;br /&gt;
The algorithm I used is pretty basic as natural language processing goes. It has two main steps, preproccesing the text and classification.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;&lt;br /&gt;
First I convert the tweets into vectors using a bigram model. This means that each unique word and pair of words becomes its own feature.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#This tweet&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;I am a drama llama&#39;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Produces these features&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;I&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;am&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;drama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;llama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;I am&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;am a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;a drama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;drama llama&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Our feature space is contructed by feeding all of our training data into this model. To convert a tweet to a vector all we do is make a vector which counts how many times each feature occurs in the tweet multiplied by two scalar terms called the term frequency and the inverse document frequency (idf).&lt;br /&gt;
The term frequency of a feature is simply the frequency of the feature in the current document(tweet). The rational behind the tf is that words that occur more might matter more. The inverse document frequency is defined as follows,&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/idf.png&quot; /&gt; 
&lt;/p&gt;
&lt;p&gt;where N is the total number of documents and &lt;em&gt;df&lt;/em&gt; is the number of documents containing our feature. As far as I can tell the &lt;em&gt;log&lt;/em&gt; is only there to keep the size of the idf from exploding. The rational behind this term is that features that occur in multiple documents are more likely to provide important clues about the document so they should be weighed higher. “But wait,”, you ask, “what about word like ‘the’, they occur everywhere but are essentially meaningless.” Well the answer is that we ignore the words that occur in too many documents and too few documents. In this way wors like ‘the’ will be ignored because they are too frequent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;br /&gt;
Now that we have high dimensional representions for the tweets as vectors we just need a way to map those vectors to a sentiment (good or bad in this case). I’ve chosen to a support vector machine to classify the tweets. A support vector machine just finds the hyperplane which best divides the two labeled sets of tweet vectors (good and bad). Then we classify new tweets as good or bad based on which side of the plane they are on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;br /&gt;
Below is the full source code. Feel free to run it yourself. You’ll need the sentiment140 data to do so as well as numpy, sci-py and, scikit-learn.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#the parameters min_df and max_df where choosen experimentally&lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#the hinge parameter specfices that we are using an svm&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;tfidf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngram_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;ISO-8859-1&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.002&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
		&lt;span class=&quot;n&quot;&gt;min_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00015&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;clf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hinge&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;l2&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_clf&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data/training.1600000.processed.noemoticon.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data/testdata.manual.2009.06.14.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;query&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;c&quot;&gt;#In the data there are three classes of data [0:negative, 2:neutral, 4:positive] &lt;/span&gt;
	&lt;span class=&quot;c&quot;&gt;#For this example we are only going to tag things are positive or negative&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loading Data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Constructing Model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Predicting Output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Enter Some Phrases to Test the Sentiment Analyzer or Type &#39;quit&#39; to Exit.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;raw_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;gt;&amp;gt;&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;quit&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;phrase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;class:&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Improvements&lt;/strong&gt;&lt;br /&gt;
This solution is only 80% accurate on the test set. That is a significant improvement on the 50% chance of guessing randomly but there is still a long way to go. Possible improvements could include, taking word order into account, uzing something like &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;word2vec&lt;/a&gt; to understand the meanings of words better, using a more complex classifier than a svm.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jul 2016 02:40:40 -0700</pubDate>
        <link>http://yourdomain.com/sentiment/analysis/logistic/regression/n-gram/2016/07/14/sentiment.html</link>
        <guid isPermaLink="true">http://yourdomain.com/sentiment/analysis/logistic/regression/n-gram/2016/07/14/sentiment.html</guid>
        
        
        <category>sentiment</category>
        
        <category>analysis</category>
        
        <category>logistic</category>
        
        <category>regression</category>
        
        <category>n-gram</category>
        
      </item>
    
      <item>
        <title>The Count of Monte Carlo</title>
        <description>&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/tree_comic.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;A couple days ago started building a small chess engine. This weekend I decided to give it a web interface. I ended up using &lt;a href=&quot;http://flask.pocoo.org/&quot;&gt;Flask&lt;/a&gt; for the backend because I like python. You can play a slighly less intelligent version (due to memory constraints on server) &lt;a href=&quot;http://playchess-pstefek.rhcloud.com/&quot;&gt;here&lt;/a&gt;. You can also view the source code on &lt;a href=&quot;https://github.com/Mr4k/playchess&quot;&gt;github&lt;/a&gt; but be warned, I have not cleaned it up yet. Although I have a working application I’m far from done. The algorithm does not play very well right now. The moves it generates make some sense but it usually slips up if you pressure it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Current Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current algorithm I am using is pretty simple. There are two main components, search and evaluation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Search Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Right now I am using a search algorithm called minimax along with a technique called alpha beta pruning. The basic principle behind 2 player minimax is that the best move is the one which minimizes your opponent’s potential gain. Before we look at how this applies to chess let’s look at a very simple example.&lt;/p&gt;

&lt;p&gt;In traditional math style, Alice and Bob are bored at home and decided to play chess. Because Alice is a chess prodigy, she destroys Bob five to nothing. To make him feel better, Alice challenges him to a new game called Trenchwaffle which heavily favors Bob. In Trenchwaffle each player can either dig a trench (branch right) or eat a waffle (branch left). There are only three turns in the whole game (Bob moves, then Alice moves, then Bob moves once more). Although the rules of Trenchwaffle are vague and mysterious we manage to procure a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Game_tree&quot;&gt;game tree&lt;/a&gt;. So the question is should Bob eat a waffle (branch left) or dig a trench (branch right)? The following diagram shows how to use the minimax algorithm to figure out which move Bob should make:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;/trenchwaffle.png&quot; /&gt; 
&lt;/p&gt;

&lt;p&gt;According to minimax, Bob should eat a waffle (branch left) because he will always win no matter what Alice chooses to do. Because Trenchwaffle is &lt;a href=&quot;https://en.wikipedia.org/wiki/Solved_game&quot;&gt;solvable&lt;/a&gt; minimax will find a winning strategy for Bob if one exists. So why does this not work with chess, isn’t &lt;a href=&quot;https://en.wikipedia.org/wiki/Solving_chess&quot;&gt;chess solvable&lt;/a&gt;? The answer is that the game tree for chess is far too big to compute in full. The average &lt;a href=&quot;https://en.wikipedia.org/wiki/Branching_factor&quot;&gt;branching factor&lt;/a&gt; for chess is around 36. So the sixth layer of the game tree contains around 36&lt;sup&gt;6&lt;/sup&gt; moves. The tenth layer contains around 36&lt;sup&gt;10&lt;/sup&gt; moves.&lt;/p&gt;

&lt;p&gt;To apply minimax to a game like chess where there are too many moves to look at we only look a small number of moves ahead and then guess how good each leaf node of our game tree is. In my current implementation I look 5 half moves ahead and even then I still need to reduce the number of nodes searched using &lt;a href=&quot;https://www.youtube.com/watch?v=Ewh-rF7KSEg&quot;&gt;alpha beta pruning&lt;/a&gt;. So now we just need a way to determine how good a position is for a particular player. This is called the evaluation function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Evaluation Function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The evaluation function takes a game state as a parameter and returns a number which represents how good that state is for one of the two players. My evaluation function takes a chess board and returns how good it thinks black is doing (larger is better).&lt;/p&gt;

&lt;p&gt;The current evalution function I am using looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function evaluate(board)
  return (total value of pieces black captured) - (total value of pieces white captured)
   + 0.1 * (total value of pieces black is attacking) + 0.1 * (total value of pieces black is defending)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It’s defintely not great and I am sure someone who was good at chess could write me a better one. However I have some other ideas to improve the evaluation function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Future&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While I think I will hold onto the search and evaluate model, I think there are a number of ways to improve both components.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improving Search&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The draw back to minimax in any form is that it is slow. By that I mean it takes about 2 minutes to search 5 half moves ahead even with alpha beta pruning. This also means it can’t look super far ahead to set up any mind blowing traps. There are several options I have to improve the search. 1. Figure out how to better prune the branches. One way to do this might be to put nodes through an evaluation function to see if they are worth exploring. 2. Use a different algorithm. The Monte Carlo Search Tree Algorithm (for which this post is named) looks promising.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improving the evaluation function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is where I might try machine learning. Chess is a nuanced game. A well placed knight could be worth losing a rook maybe even a queen but it’s usually not. I’m hoping that thousands of previous chess games describe these situations better than I can.&lt;/p&gt;

</description>
        <pubDate>Tue, 28 Jun 2016 13:30:40 -0700</pubDate>
        <link>http://yourdomain.com/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html</link>
        <guid isPermaLink="true">http://yourdomain.com/chess/project/minimax/algorithm/search/monte/carlo/2016/06/28/search.html</guid>
        
        
        <category>chess</category>
        
        <category>project</category>
        
        <category>minimax</category>
        
        <category>algorithm</category>
        
        <category>search</category>
        
        <category>monte</category>
        
        <category>carlo</category>
        
      </item>
    
  </channel>
</rss>
